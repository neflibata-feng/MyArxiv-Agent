import datetime
import urllib.parse
import feedparser
import os

# é…ç½®ï¼šéœ€è¦å…³æ³¨çš„ arXiv åˆ†ç±»
CATEGORIES = ["cs.AI"]
# å…³é”®è¯è¿‡æ»¤ (æ ‡é¢˜æˆ–æ‘˜è¦ä¸­å¿…é¡»åŒ…å«)
KEYWORDS = ["Agent"]

# è·å–é¡¹ç›®æ ¹ç›®å½• (å‡è®¾ script åœ¨ scripts/ ç›®å½•ä¸‹)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

def fetch_papers():
    """
    çˆ¬å– arXiv æ•°æ®
    """
    print(f"è·å–æ—¥æœŸä¸º {datetime.date.today()}...")
    
    base_url = 'http://export.arxiv.org/api/query?'
    
    # æ„å»ºæŸ¥è¯¢
    cat_query = ' OR '.join([f'cat:{c}' for c in CATEGORIES])
    kw_query = ' OR '.join([f'all:{k}' for k in KEYWORDS])
    
    search_query = f'({cat_query}) AND ({kw_query})'
    
    params = {
        'search_query': search_query,
        'start': 0,
        'max_results': 150,  # å¢åŠ åˆ° 150 ä»¥ç¡®ä¿ä¸é—æ¼
        'sortBy': 'submittedDate',
        'sortOrder': 'descending'
    }
    
    query_string = urllib.parse.urlencode(params)
    url = base_url + query_string
    print(f"æŸ¥è¯¢é“¾æ¥ä¸º: {url}")
    
    try:
        feed = feedparser.parse(url)
    except Exception as e:
        print(f"è·å–æ•°æ®é”™è¯¯: {e}")
        return []

    papers = []
    for entry in feed.entries:
        try:
            # æå–ä¿¡æ¯
            title = entry.title.replace('\n', ' ').strip()
            link = entry.link
            
            # primary_category
            if hasattr(entry, 'arxiv_primary_category'):
                category = entry.arxiv_primary_category['term']
            else:
                category = 'Unknown'
            
            # Authors
            authors = [a.name for a in entry.authors]
            if len(authors) > 1:
                author_str = f"{authors[0]} et al."
            elif len(authors) == 1:
                author_str = authors[0]
            else:
                author_str = "Unknown"

            # Published Date
            if hasattr(entry, 'published_parsed'):
                pub_date = datetime.date(*entry.published_parsed[:3]).strftime("%Y-%m-%d")
            else:
                pub_date = "Unknown Date"
            
            # Summary
            summary = entry.summary.replace('\n', ' ').strip()
            # ç®€å•æ¸…ç† LaTeX æ ‡è®°
            summary_hint = summary[:250] + "..." if len(summary) > 250 else summary
            
            papers.append({
                'title': title,
                'link': link,
                'category': category,
                'summary': summary_hint,
                'published': pub_date,
                'author': author_str
            })
        except Exception as e:
            print(f"Skipping entry due to error: {e}")
            continue
            
    return papers

def update_inbox(papers):
    """
    å°†æ–°è®ºæ–‡è¿½åŠ åˆ° Inbox.md å¤´éƒ¨ (å¸¦å»é‡åŠŸèƒ½)
    """
    if not papers:
        print("æ²¡æœ‰è®ºæ–‡æ›´æ–°")
        return

    file_path = os.path.join(BASE_DIR, "Inbox.md")
    today_str = datetime.date.today().strftime("%Y-%m-%d")
    
    # --- å»é‡é€»è¾‘ ---
    existing_links = set()
    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
            for p in papers:
                if p['link'] in content:
                    existing_links.add(p['link'])
    
    new_papers = [p for p in papers if p['link'] not in existing_links]
    
    if not new_papers:
        print("æ²¡æœ‰è®ºæ–‡æ›´æ–°")
        return

    print(f"è·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡. å…¶ä¸­{len(new_papers)} ç¯‡æ˜¯æ–°çš„")
    # ----------------
    
    new_lines = []
    new_lines.append(f"## {today_str} æ›´æ–° {len(new_papers)} ç¯‡æ–°è®ºæ–‡\n")
    for p in new_papers:
        # Markdown æ ¼å¼ä¼˜åŒ–
        line = f"- [ ] **[{p['category']}]** [{p['title']}]({p['link']}) *by {p['author']} ({p['published']})* - _{p['summary']}_\n"
        new_lines.append(line)
    new_lines.append("\n")

    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            old_lines = f.readlines()
    else:
        old_lines = ["# ğŸ“¥ My Arxiv Inbox\n\n", "è¿™é‡Œæ˜¯ä½ çš„å¾…é˜…è¯»åŒºã€‚\n\n", "---\n\n"]

    insert_index = -1
    for i, line in enumerate(old_lines):
        if line.strip() == "---":
            insert_index = i + 1
            break
    
    if insert_index == -1:
        old_lines.append("\n---\n")
        insert_index = len(old_lines)

    final_lines = old_lines[:insert_index] + ["\n"] + new_lines + old_lines[insert_index:]

    with open(file_path, "w", encoding="utf-8") as f:
        f.writelines(final_lines)
    
    print(f"æˆåŠŸæ·»åŠ  {len(new_papers)} ç¯‡è®ºæ–‡è‡³ {file_path}")

if __name__ == "__main__":
    papers = fetch_papers()
    update_inbox(papers)
